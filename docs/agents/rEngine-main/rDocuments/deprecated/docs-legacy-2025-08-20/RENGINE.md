# rEngine: AI-Powered Development Platform v2.1.0

## Overview

rEngine is StackTrackr's centralized AI orchestration and development platform that powers our entire application ecosystem. It serves as both our containerized infrastructure and multi-provider AI intelligence layer, providing a unified system for memory management, agent coordination, and AI-driven features.

## ðŸ§  Current Architecture (August 2025)

### AI Provider Strategy (5-Tier Fallback System)

- **Primary Intelligence**: Groq API
  - Ultra-fast response (50+ tokens/sec)
  - Cost-effective for high-volume operations
  - Primary provider for development tasks
  - Llama 3.1 70B model for reasoning

- **Intelligent Fallback Chain**
  1. **Groq** â†’ Fast primary responses
  2. **Claude** â†’ Complex reasoning when Groq unavailable  
  3. **OpenAI** â†’ General intelligence backup
  4. **Gemini** â†’ Google Search grounding for market intelligence
  5. **Ollama** â†’ Local fallback (Qwen 2.5 Coder 3B)

- **Memory Integration**
  - MCP (Model Context Protocol) for persistent knowledge
  - Cross-session learning and context preservation
  - Intelligent handoff between providers
  - Automatic fallback when APIs unavailable

### Container Architecture (Docker v2.1.0)

```mermaid
graph TD
    A[nginx:3032] --> B[stacktrackr-app:3033]
    A --> C[rengine-platform:3034]
    A --> D[mcp-server:3036]
    C --> E[5-Tier AI System]
    C --> F[Memory System]
    D --> G[Persistent Knowledge]
```

#### Service Configuration

1. **nginx** (Port 3032)
   - Reverse proxy and entry point
   - Routes to all backend services
   - SSL termination ready

1. **stacktrackr-app** (Port 3033)
   - Main StackTrackr application
   - React frontend with API integration
   - Hot-reload development ready

1. **rengine-platform** (Port 3034-3035)
   - AI orchestration and agent management
   - Multi-provider API routing
   - Memory management and context preservation
   - WebSocket support for real-time features

1. **mcp-server** (Port 3036)
   - Model Context Protocol server
   - Persistent memory across sessions
   - Cross-project knowledge sharing
   - Agent context management
   - Optimization insights
   - Handoff protocols

## ðŸ”„ Current Integration Status

### Mobile Development System (v2.1.0)

- **Checkout/Checkin System**: Complete portable development environments
- **API Fallback**: Cloud APIs when Ollama unavailable  
- **Security Protected**: Git-ignored files never synced to GitHub
- **Conflict Resolution**: Smart merge with backup protection

### Docker Infrastructure (Production Ready)

```yaml

# Current Port Allocation (No Conflicts)

services:
  nginx: 3032:80, 3038:443      # Entry point
  stacktrackr-app: 3033:3000    # Main application  
  rengine-platform: 3034:8080, 3035:8081  # AI services
  mcp-server: 3036:8082         # Memory system
  development: 3037:8000        # Development tools
```

### Key Features Implemented

1. **Multi-Provider AI Intelligence**
   - Groq primary (50+ tokens/sec)
   - Claude fallback for complex reasoning
   - OpenAI backup for general intelligence  
   - Gemini for market intelligence (planned)
   - Ollama local fallback (Qwen 2.5 Coder)

1. **Memory Persistence**
   - MCP server for cross-session knowledge
   - Agent context preservation
   - Development pattern learning
   - Automatic handoff protocols

1. **Development Acceleration**
   - One-click Docker environment
   - Mobile development packages
   - API key protection and fallbacks
   - Intelligent caching and optimization

## ðŸ“‚ Current Directory Structure (August 2025)

```text
rEngine/
â”œâ”€â”€ enhanced-agent-init.js        # Multi-provider AI initialization
â”œâ”€â”€ recall.js                     # Memory system interface
â”œâ”€â”€ memory-intelligence.js        # Context management
â”œâ”€â”€ one-click-startup.js          # Docker integration
â”œâ”€â”€ package.json                  # v2.1.0 configuration
â””â”€â”€ .env                          # API keys (git-ignored)

StackTrackr/
â”œâ”€â”€ docker-compose.yml            # Container orchestration
â”œâ”€â”€ docker-dev.sh                 # Management scripts
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ mobile-checkout.js        # Portable development
â”‚   â”œâ”€â”€ mobile-checkin.js         # Change merging
â”‚   â””â”€â”€ docker-requirement-check.sh
â”œâ”€â”€ js/rengine-api-client.js      # Frontend integration
â””â”€â”€ docs/                         # Complete documentation
```

## ðŸš€ Current Development Workflow (Production Ready)

### Container Management

```bash

# Start complete development environment

./docker-dev.sh start

# Access at: http://localhost:3032

# Individual service access

# Main App: http://localhost:3033

# rEngine API: http://localhost:3034  

# MCP Server: http://localhost:3036

```

### Mobile Development

```bash

# Create portable development package

npm run mobile-checkout

# -> Creates mobile-checkout-{timestamp}.zip (29MB)

# On remote machine

unzip mobile-checkout-*.zip && ./mobile-setup.sh

# Full development environment ready!

# Merge changes back

npm run mobile-checkin mobile-checkout-{timestamp}

# Smart conflict resolution and backup protection

```

### AI Provider Testing

```bash

# Test all providers in fallback chain

cd rEngine && node test-ai-providers.js

# -> Tests Groq â†’ Claude â†’ OpenAI â†’ Gemini â†’ Ollama

# Quick system check

node enhanced-agent-init.js

# -> Initializes with optimal provider selection

```

## ðŸ“Š Current Performance Metrics (August 2025)

### AI Response Times

- **Groq Primary**: 50+ tokens/sec (Lightning fast)
- **Claude Fallback**: 15-25 tokens/sec (High quality)
- **OpenAI Backup**: 10-20 tokens/sec (Reliable)
- **Ollama Local**: 5-15 tokens/sec (Always available)

### Development Impact

- **Docker Setup**: 0 conflicts, 30-second startup
- **Mobile Packages**: 29MB complete environments
- **API Fallback**: 99.9% uptime through redundancy
- **Memory Persistence**: Cross-session context preservation

### Resource Optimization

- **Primary Costs**: Groq API (~$5-50/month)
- **Fallback Costs**: Usage-based, automatic optimization
- **Local Fallback**: Zero cost, always available
- **Container Overhead**: Minimal, optimized builds

## ðŸŽ¯ Strategic Advantages (Realized)

1. **Development Velocity**
   - Zero-conflict Docker environment
   - Portable development packages  
   - AI-powered code assistance
   - Persistent knowledge across sessions

1. **Reliability Through Redundancy**
   - 5-tier AI fallback system
   - Local backup (Ollama) always available
   - Smart provider selection based on task
   - Automatic failover and recovery

1. **Cost Optimization**
   - Groq primary: Fast and affordable
   - Intelligent provider routing
   - Local fallback eliminates API dependency
   - Usage optimization through caching

1. **Mobile Development Capability**
   - Work anywhere with full environment
   - Secure API key handling
   - Conflict-free change merging
   - Professional development workflow

## ðŸ”œ Future Development (Strategic Vision)

1. **Enhanced Market Intelligence** (Q4 2025)
   - Gemini integration for real-time pricing
   - Google Search grounding for market data
   - Competitive intelligence automation
   - Advanced analytics and predictions

1. **Platform Expansion** (2026)
   - Multi-project support under rEngine umbrella
   - Shared template library
   - Enterprise containerization
   - Advanced monitoring and analytics

The rEngine Platform v2.1.0 represents a mature, production-ready development environment that combines the best of AI intelligence, container orchestration, and mobile development workflows. By implementing a robust 5-tier AI system with Groq as primary provider, we've achieved both performance and reliability while maintaining cost efficiency and local fallback capabilities.
