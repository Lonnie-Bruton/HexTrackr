#!/bin/bash

# AI Memory System Review Benchmark
# Tests all available LLMs with identical prompt to review the memory system setup

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(dirname "$SCRIPT_DIR")"
TIMESTAMP=$(date +"%Y%m%d_%H%M%S")
OUTPUT_DIR="$PROJECT_ROOT/benchmark_results/memory_review_$TIMESTAMP"

# Create output directory
mkdir -p "$OUTPUT_DIR"

# Load API keys
if [ -f "$PROJECT_ROOT/rEngine/.env" ]; then
    source "$PROJECT_ROOT/rEngine/.env"
else
    echo "Error: API keys file not found at $PROJECT_ROOT/rEngine/.env"
    exit 1
fi

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

echo -e "${BLUE}=== AI Memory System Review Benchmark ===${NC}"
echo "Timestamp: $(date)"
echo "Output Directory: $OUTPUT_DIR"
echo ""

# The exact same prompt for all AIs
REVIEW_PROMPT="Please review all the code, architecture, and implementation of this memory system setup for AI agents. I need your honest, critical assessment.

**What to Review:**
- Memory Scribe dashboard with real-time console monitoring
- MCP server integration and management  
- Console log monitoring and AI agent activity tracking
- 15-minute automated health checks with desktop alerts
- Port management and conflict resolution
- Shared memory files vs personal agent memory files
- API endpoints for memory system access
- File monitoring, console interception, and activity logging

**Specific Questions:**
1. What are the biggest weaknesses in this memory system?
2. What could fail catastrophically?
3. What's overcomplicated or unnecessary?
4. What critical gaps do you see?
5. How would you improve the architecture?
6. What security concerns exist?
7. Rate the overall design (1-10) and explain why

Be brutally honest. Point out flaws, suggest improvements, identify risks. This is for system improvement, not ego stroking."

# Function to test Ollama models
test_ollama() {
    local model="$1"
    local output_file="$OUTPUT_DIR/ollama_${model}_review.md"
    
    echo -e "${YELLOW}Testing Ollama model: $model${NC}"
    
    local start_time=$(date +%s)
    
    # Create the request payload with proper JSON escaping
    local json_payload=$(jq -n \
        --arg model "$model" \
        --arg prompt "$REVIEW_PROMPT" \
        '{
            model: $model,
            prompt: $prompt,
            stream: false,
            options: {
                temperature: 0.7,
                top_p: 0.9,
                max_tokens: 4000
            }
        }')
    
    local response=$(curl -s --max-time 120 \
        -H "Content-Type: application/json" \
        -d "$json_payload" \
        http://localhost:11434/api/generate)
    
    local end_time=$(date +%s)
    local duration=$((end_time - start_time))
    
    if [ $? -eq 0 ] && [ -n "$response" ]; then
        # Extract the response text
        local review_text=$(echo "$response" | jq -r '.response // empty')
        
        if [ -n "$review_text" ] && [ "$review_text" != "null" ]; then
            # Create markdown output
            cat > "$output_file" << EOF
# Ollama $model - Memory System Review

**Model:** $model
**Duration:** ${duration}s
**Timestamp:** $(date)

## Review Response

$review_text

---
*Generated by Ollama $model via local API*
EOF
            echo -e "${GREEN}✓ $model completed review (${duration}s)${NC}"
            return 0
        else
            echo -e "${RED}✗ $model failed - empty response${NC}"
            return 1
        fi
    else
        echo -e "${RED}✗ $model failed - API error${NC}"
        return 1
    fi
}

# Function to test OpenAI
test_openai() {
    local model="gpt-4o"
    local output_file="$OUTPUT_DIR/openai_${model}_review.md"
    
    echo -e "${YELLOW}Testing OpenAI model: $model${NC}"
    
    if [ -z "$OPENAI_API_KEY" ]; then
        echo -e "${RED}✗ OpenAI API key not found${NC}"
        return 1
    fi
    
    local start_time=$(date +%s)
    
    local json_payload=$(jq -n \
        --arg model "$model" \
        --arg prompt "$REVIEW_PROMPT" \
        '{
            model: $model,
            messages: [
                {
                    role: "user",
                    content: $prompt
                }
            ],
            max_tokens: 4000,
            temperature: 0.7
        }')
    
    local response=$(curl -s --max-time 120 \
        -H "Content-Type: application/json" \
        -H "Authorization: Bearer $OPENAI_API_KEY" \
        -d "$json_payload" \
        https://api.openai.com/v1/chat/completions)
    
    local end_time=$(date +%s)
    local duration=$((end_time - start_time))
    
    if [ $? -eq 0 ] && [ -n "$response" ]; then
        local review_text=$(echo "$response" | jq -r '.choices[0].message.content // empty')
        
        if [ -n "$review_text" ] && [ "$review_text" != "null" ]; then
            cat > "$output_file" << EOF
# OpenAI $model - Memory System Review

**Model:** $model
**Duration:** ${duration}s
**Timestamp:** $(date)

## Review Response

$review_text

---
*Generated by OpenAI $model via API*
EOF
            echo -e "${GREEN}✓ $model completed review (${duration}s)${NC}"
            return 0
        else
            echo -e "${RED}✗ $model failed - empty response${NC}"
            return 1
        fi
    else
        echo -e "${RED}✗ $model failed - API error${NC}"
        return 1
    fi
}

# Function to test Claude
test_claude() {
    local model="claude-3-5-sonnet-20241022"
    local output_file="$OUTPUT_DIR/claude_${model}_review.md"
    
    echo -e "${YELLOW}Testing Claude model: $model${NC}"
    
    if [ -z "$ANTHROPIC_API_KEY" ]; then
        echo -e "${RED}✗ Claude API key not found${NC}"
        return 1
    fi
    
    local start_time=$(date +%s)
    
    local json_payload=$(jq -n \
        --arg model "$model" \
        --arg prompt "$REVIEW_PROMPT" \
        '{
            model: $model,
            max_tokens: 4000,
            messages: [
                {
                    role: "user",
                    content: $prompt
                }
            ]
        }')
    
    local response=$(curl -s --max-time 120 \
        -H "Content-Type: application/json" \
        -H "x-api-key: $ANTHROPIC_API_KEY" \
        -H "anthropic-version: 2023-06-01" \
        -d "$json_payload" \
        https://api.anthropic.com/v1/messages)
    
    local end_time=$(date +%s)
    local duration=$((end_time - start_time))
    
    if [ $? -eq 0 ] && [ -n "$response" ]; then
        local review_text=$(echo "$response" | jq -r '.content[0].text // empty')
        
        if [ -n "$review_text" ] && [ "$review_text" != "null" ]; then
            cat > "$output_file" << EOF
# Claude $model - Memory System Review

**Model:** $model  
**Duration:** ${duration}s
**Timestamp:** $(date)

## Review Response

$review_text

---
*Generated by Claude $model via Anthropic API*
EOF
            echo -e "${GREEN}✓ $model completed review (${duration}s)${NC}"
            return 0
        else
            echo -e "${RED}✗ $model failed - empty response${NC}"
            return 1
        fi
    else
        echo -e "${RED}✗ $model failed - API error${NC}"
        return 1
    fi
}

# Function to test Gemini
test_gemini() {
    local model="gemini-1.5-pro"
    local output_file="$OUTPUT_DIR/gemini_${model}_review.md"
    
    echo -e "${YELLOW}Testing Gemini model: $model${NC}"
    
    if [ -z "$GOOGLE_API_KEY" ]; then
        echo -e "${RED}✗ Gemini API key not found${NC}"
        return 1
    fi
    
    local start_time=$(date +%s)
    
    local json_payload=$(jq -n \
        --arg prompt "$REVIEW_PROMPT" \
        '{
            contents: [
                {
                    parts: [
                        {
                            text: $prompt
                        }
                    ]
                }
            ],
            generationConfig: {
                temperature: 0.7,
                maxOutputTokens: 4000
            }
        }')
    
    local response=$(curl -s --max-time 120 \
        -H "Content-Type: application/json" \
        -d "$json_payload" \
        "https://generativelanguage.googleapis.com/v1beta/models/$model:generateContent?key=$GOOGLE_API_KEY")
    
    local end_time=$(date +%s)
    local duration=$((end_time - start_time))
    
    if [ $? -eq 0 ] && [ -n "$response" ]; then
        local review_text=$(echo "$response" | jq -r '.candidates[0].content.parts[0].text // empty')
        
        if [ -n "$review_text" ] && [ "$review_text" != "null" ]; then
            cat > "$output_file" << EOF
# Gemini $model - Memory System Review

**Model:** $model
**Duration:** ${duration}s  
**Timestamp:** $(date)

## Review Response

$review_text

---
*Generated by Gemini $model via Google API*
EOF
            echo -e "${GREEN}✓ $model completed review (${duration}s)${NC}"
            return 0
        else
            echo -e "${RED}✗ $model failed - empty response${NC}"
            return 1
        fi
    else
        echo -e "${RED}✗ $model failed - API error${NC}"
        return 1
    fi
}

# Main execution
echo "Starting memory system review benchmark..."
echo ""

# Test local Ollama models first
echo -e "${BLUE}Testing Local Ollama Models:${NC}"
ollama_models=("qwen2.5:3b" "gemma2:2b" "llama3:8b")
for model in "${ollama_models[@]}"; do
    test_ollama "$model"
done

echo ""

# Test cloud APIs
echo -e "${BLUE}Testing Cloud APIs:${NC}"
test_openai
test_claude  
test_gemini

echo ""
echo -e "${BLUE}=== Benchmark Complete ===${NC}"
echo "Results saved to: $OUTPUT_DIR"

# Create summary
echo "Generating summary..."
cat > "$OUTPUT_DIR/README.md" << EOF
# AI Memory System Review Results

**Benchmark Run:** $(date)
**Prompt:** Same exact review prompt sent to all models
**Purpose:** Get honest, critical assessment of memory system architecture

## Files Generated

EOF

# List all generated files
for file in "$OUTPUT_DIR"/*.md; do
    if [ "$file" != "$OUTPUT_DIR/README.md" ]; then
        echo "- $(basename "$file")" >> "$OUTPUT_DIR/README.md"
    fi
done

echo ""
echo "Review complete! Check the results:"
echo "cd $OUTPUT_DIR && ls -la"
echo ""
echo "To read all reviews:"
echo "cat $OUTPUT_DIR/*.md | less"
