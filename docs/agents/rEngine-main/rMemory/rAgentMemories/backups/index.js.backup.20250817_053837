#!/usr/bin/env node

/**
 * rEngineMCP - VS Code Chat Integration MCP Server
 * Enhanced Multi-Provider AI Memory Scribe for VS Code Copilot Chat
 * 5-Tier Intelligent Fallback: Groq → Claude → ChatGPT → Gemini → Ollama
 * 
 * @version 2.1.0
 * @author lbruton
 * @date 2025-08-17
 */

import { Server } from '@modelcontextprotocol/sdk/server/index.js';
import { StdioServerTransport } from '@modelcontextprotocol/sdk/server/stdio.js';
import {
  CallToolRequestSchema,
  ErrorCode,
  ListToolsRequestSchema,
  McpError,
} from '@modelcontextprotocol/sdk/types.js';
import axios from 'axios';
import fs from 'fs-extra';
import path from 'path';
import { fileURLToPath } from 'url';
import { GoogleGenerativeAI } from '@google/generative-ai';
import Anthropic from '@anthropic-ai/sdk';
import OpenAI from 'openai';

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

// Environment configuration
const GROQ_API_KEY = process.env.GROQ_API_KEY;
const GEMINI_API_KEY = process.env.GEMINI_API_KEY;
const ANTHROPIC_API_KEY = process.env.ANTHROPIC_API_KEY;
const OPENAI_API_KEY = process.env.OPENAI_API_KEY;
const OLLAMA_URL = process.env.OLLAMA_URL || 'http://localhost:11434';

// VS Code Chat Integration Logger
function logToVSCode(message, level = 'info') {
  const timestamp = new Date().toISOString();
  console.error(`[${timestamp}] [rEngineMCP-VSCode] ${level.toUpperCase()}: ${message}`);
}

// 5-Tier AI Provider Configuration
const AI_MODELS = {
  groq: {
    provider: 'groq',
    model: 'llama-3.1-8b-instant',
    apiUrl: 'https://api.groq.com/openai/v1/chat/completions',
    maxTokens: 8000,
    priority: 1
  },
  claude: {
    provider: 'anthropic',
    model: 'claude-3-haiku-20240307',
    maxTokens: 4000,
    priority: 2
  },
  openai: {
    provider: 'openai',
    model: 'gpt-3.5-turbo',
    maxTokens: 4000,
    priority: 3
  },
  gemini: {
    provider: 'google',
    model: 'gemini-1.5-flash',
    maxTokens: 8000,
    priority: 4
  },
  ollama: {
    provider: 'ollama',
    models: ['qwen2.5-coder:3b', 'gemma2:2b', 'qwen2.5:3b', 'qwen2:7b', 'llama3:8b'],
    maxTokens: 4000,
    priority: 5
  }
};

// Initialize AI providers
let anthropic = null;
let openai = null;
let gemini = null;

if (ANTHROPIC_API_KEY) {
  anthropic = new Anthropic({ apiKey: ANTHROPIC_API_KEY });
}

if (OPENAI_API_KEY) {
  openai = new OpenAI({ apiKey: OPENAI_API_KEY });
}

if (GEMINI_API_KEY) {
  gemini = new GoogleGenerativeAI(GEMINI_API_KEY);
}

// Memory Manager for VS Code Integration
class VSCodeMemoryManager {
  constructor() {
    this.memoryDir = path.join(__dirname, '.rengine', 'memory');
    this.conversationsDir = path.join(__dirname, '.rengine', 'conversations');
    this.agentsDir = path.join(__dirname, '..', 'agents');
    this.initializeDirectories();
  }

  async initializeDirectories() {
    await fs.ensureDir(this.memoryDir);
    await fs.ensureDir(this.conversationsDir);
    logToVSCode('Memory directories initialized for VS Code integration');
  }

  async saveToAgents(content, operation = 'unknown') {
    try {
      const timestamp = new Date().toISOString();
      const entry = {
        timestamp,
        operation,
        content,
        source: 'vscode-chat'
      };

      // Save to agents memory system
      const memoryPath = path.join(this.agentsDir, 'memory.json');
      let memories = [];
      
      if (await fs.pathExists(memoryPath)) {
        memories = await fs.readJson(memoryPath);
      }
      
      memories.push(entry);
      await fs.writeJson(memoryPath, memories, { spaces: 2 });
      
      logToVSCode(`Saved ${operation} to agents memory system`);
    } catch (error) {
      logToVSCode(`Error saving to agents: ${error.message}`, 'error');
    }
  }
}

// Enhanced AI calling functions for VS Code Chat
async function callGroqAPI(messages, model = AI_MODELS.groq.model) {
  if (!GROQ_API_KEY) throw new Error('Groq API key not configured');
  
  const response = await axios.post(AI_MODELS.groq.apiUrl, {
    model,
    messages,
    max_tokens: AI_MODELS.groq.maxTokens,
    temperature: 0.7
  }, {
    headers: {
      'Authorization': `Bearer ${GROQ_API_KEY}`,
      'Content-Type': 'application/json'
    },
    timeout: 30000
  });

  return {
    content: response.data.choices[0].message.content,
    provider: 'groq',
    model,
    success: true
  };
}

async function callClaudeAPI(messages, model = AI_MODELS.claude.model) {
  if (!anthropic) throw new Error('Claude API not configured');
  
  const systemMessage = messages.find(m => m.role === 'system');
  const userMessages = messages.filter(m => m.role !== 'system');
  
  const response = await anthropic.messages.create({
    model,
    max_tokens: AI_MODELS.claude.maxTokens,
    system: systemMessage?.content || '',
    messages: userMessages
  });

  return {
    content: response.content[0].text,
    provider: 'claude',
    model,
    success: true
  };
}

async function callOpenAIAPI(messages, model = AI_MODELS.openai.model) {
  if (!openai) throw new Error('OpenAI API not configured');
  
  const response = await openai.chat.completions.create({
    model,
    messages,
    max_tokens: AI_MODELS.openai.maxTokens,
    temperature: 0.7
  });

  return {
    content: response.choices[0].message.content,
    provider: 'openai',
    model,
    success: true
  };
}

async function callGeminiAPI(messages, model = AI_MODELS.gemini.model) {
  if (!gemini) throw new Error('Gemini API not configured');
  
  const genModel = gemini.getGenerativeModel({ model });
  const prompt = messages.map(m => `${m.role}: ${m.content}`).join('\n\n');
  
  const result = await genModel.generateContent(prompt);
  const response = await result.response;
  
  return {
    content: response.text(),
    provider: 'gemini',
    model,
    success: true
  };
}

async function callOllamaAPI(messages, selectedModel = null) {
  const availableModels = await getAvailableOllamaModels();
  const model = selectedModel || availableModels[0] || 'llama3:8b';
  
  const response = await axios.post(`${OLLAMA_URL}/api/chat`, {
    model,
    messages,
    stream: false
  }, { timeout: 120000 });

  return {
    content: response.data.message.content,
    provider: 'ollama',
    model,
    success: true
  };
}

async function getAvailableOllamaModels() {
  try {
    const response = await axios.get(`${OLLAMA_URL}/api/tags`, { timeout: 5000 });
    return response.data.models.map(m => m.name);
  } catch (error) {
    logToVSCode(`Failed to get Ollama models: ${error.message}`, 'warn');
    return [];
  }
}

// Enhanced tagging with 5-tier fallback for VS Code Chat
async function enhancedAIAnalysis(content, operation = 'analyze') {
  const timestamp = new Date().toISOString();
  const messages = [
    {
      role: 'system',
      content: `You are an intelligent analysis assistant integrated with VS Code Chat. Analyze the following content for a precious metals inventory system. Provide insights, suggestions, and relevant information. Operation: ${operation}, Time: ${timestamp}`
    },
    {
      role: 'user',
      content: `Please analyze: ${content}`
    }
  ];

  const providers = [
    { name: 'groq', func: callGroqAPI },
    { name: 'claude', func: callClaudeAPI },
    { name: 'openai', func: callOpenAIAPI },
    { name: 'gemini', func: callGeminiAPI },
    { name: 'ollama', func: callOllamaAPI }
  ];

  // Try each provider in priority order
  for (const provider of providers) {
    try {
      logToVSCode(`Attempting ${provider.name.toUpperCase()} for VS Code Chat analysis...`);
      const result = await provider.func(messages);
      
      if (result.success) {
        logToVSCode(`${provider.name.toUpperCase()} analysis successful for VS Code Chat`);
        
        // Save to agents memory
        await memoryManager.saveToAgents({
          input: content,
          output: result.content,
          provider: result.provider,
          model: result.model,
          operation
        }, 'ai-analysis');
        
        return {
          analysis: result.content,
          provider: result.provider,
          model: result.model,
          timestamp
        };
      }
    } catch (error) {
      logToVSCode(`${provider.name.toUpperCase()} failed, trying next provider: ${error.message}`, 'warn');
      continue;
    }
  }

  throw new Error('All AI providers failed for VS Code Chat analysis');
}

// Initialize memory manager
const memoryManager = new VSCodeMemoryManager();

// MCP Server setup for VS Code Integration
const server = new Server(
  {
    name: 'rengine-mcp-vscode',
    version: '2.1.0',
  },
  {
    capabilities: {
      tools: {},
    },
  }
);

// Tool definitions for VS Code Chat
server.setRequestHandler(ListToolsRequestSchema, async () => {
  return {
    tools: [
      {
        name: 'analyze_with_ai',
        description: 'Analyze content using 5-tier AI system with intelligent fallback for VS Code Chat',
        inputSchema: {
          type: 'object',
          properties: {
            content: {
              type: 'string',
              description: 'Content to analyze',
            },
            operation: {
              type: 'string',
              description: 'Type of analysis to perform',
              default: 'general_analysis',
            },
          },
          required: ['content'],
        },
      },
      {
        name: 'get_agents_memory',
        description: 'Retrieve information from the agents memory system',
        inputSchema: {
          type: 'object',
          properties: {
            query: {
              type: 'string',
              description: 'Search query for agents memory',
            },
            limit: {
              type: 'number',
              description: 'Number of results to return',
              default: 5,
            },
          },
        },
      },
      {
        name: 'vscode_system_status',
        description: 'Get status of VS Code MCP integration and AI providers',
        inputSchema: {
          type: 'object',
          properties: {},
        },
      },
    ],
  };
});

// Tool handlers for VS Code Chat integration
server.setRequestHandler(CallToolRequestSchema, async (request) => {
  const { name, arguments: args } = request.params;

  try {
    switch (name) {
      case 'analyze_with_ai': {
        const result = await enhancedAIAnalysis(args.content, args.operation || 'general_analysis');
        return {
          content: [
            {
              type: 'text',
              text: JSON.stringify(result, null, 2),
            },
          ],
        };
      }

      case 'get_agents_memory': {
        const memoryPath = path.join(memoryManager.agentsDir, 'memory.json');
        
        if (await fs.pathExists(memoryPath)) {
          const memories = await fs.readJson(memoryPath);
          const query = args.query?.toLowerCase() || '';
          
          const filtered = query 
            ? memories.filter(m => 
                JSON.stringify(m).toLowerCase().includes(query)
              ).slice(-(args.limit || 5))
            : memories.slice(-(args.limit || 5));
          
          return {
            content: [
              {
                type: 'text',
                text: JSON.stringify(filtered, null, 2),
              },
            ],
          };
        } else {
          return {
            content: [
              {
                type: 'text',
                text: 'No agents memory found.',
              },
            ],
          };
        }
      }

      case 'vscode_system_status': {
        const status = {
          timestamp: new Date().toISOString(),
          vscode_integration: 'active',
          providers: {
            groq: { configured: !!GROQ_API_KEY, priority: 1 },
            claude: { configured: !!ANTHROPIC_API_KEY, priority: 2 },
            openai: { configured: !!OPENAI_API_KEY, priority: 3 },
            gemini: { configured: !!GEMINI_API_KEY, priority: 4 },
            ollama: { configured: true, priority: 5, models: await getAvailableOllamaModels() }
          },
          memory: {
            agents_dir: memoryManager.agentsDir,
            memory_dir: memoryManager.memoryDir,
            conversations_dir: memoryManager.conversationsDir
          }
        };

        return {
          content: [
            {
              type: 'text',
              text: JSON.stringify(status, null, 2),
            },
          ],
        };
      }

      default:
        throw new McpError(
          ErrorCode.MethodNotFound,
          `Unknown tool: ${name}`
        );
    }
  } catch (error) {
    logToVSCode(`Error executing tool ${name}: ${error.message}`, 'error');
    throw new McpError(
      ErrorCode.InternalError,
      `Error executing tool ${name}: ${error.message}`
    );
  }
});

// VS Code MCP Server startup
async function startVSCodeMCPServer() {
  logToVSCode('Starting Enhanced MCP Server for VS Code Chat Integration...');
  logToVSCode('5-Tier System: Groq → Claude → ChatGPT → Gemini → Ollama');
  
  // Check provider availability
  const providers = [];
  if (GROQ_API_KEY) providers.push('Groq');
  if (ANTHROPIC_API_KEY) providers.push('Claude');
  if (OPENAI_API_KEY) providers.push('OpenAI');
  if (GEMINI_API_KEY) providers.push('Gemini');
  providers.push('Ollama (local)');
  
  logToVSCode(`Available providers: ${providers.join(', ')}`);
  
  // Test Ollama connection
  try {
    const models = await getAvailableOllamaModels();
    if (models.length > 0) {
      logToVSCode(`Ollama models available: ${models.join(', ')}`);
    } else {
      logToVSCode('No Ollama models found', 'warn');
    }
  } catch (error) {
    logToVSCode(`Ollama connection failed: ${error.message}`, 'warn');
  }
  
  // Initialize memory system
  await memoryManager.initializeDirectories();
  
  logToVSCode('VS Code MCP Server ready for chat integration');
}

// Start the server for VS Code
async function main() {
  await startVSCodeMCPServer();
  
  const transport = new StdioServerTransport();
  await server.connect(transport);
  logToVSCode('rEngineMCP Server connected to VS Code Chat');
}

main().catch((error) => {
  logToVSCode(`Fatal error: ${error.message}`, 'error');
  process.exit(1);
});