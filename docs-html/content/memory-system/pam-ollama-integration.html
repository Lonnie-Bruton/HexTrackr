<!-- Fragment template for documentation content injected by docs-tabler.js -->
<section class="doc-fragment">
    <h1>PAM and Ollama Embedding Integration</h1>
<blockquote>
<p><strong>üîç Overview</strong>: This document describes how Persistent AI Memory (PAM) integrates with Ollama&#39;s nomic-embed-text embedding model for semantic search capabilities.</p>
</blockquote>
<h2>Architecture</h2>
<p>The integration between PAM and Ollama consists of several components:</p>
<ol>
<li><strong>Persistent AI Memory (PAM)</strong> - The core memory system for storing unstructured memories</li>
<li><strong>Ollama Embedding Proxy</strong> - A service that translates OpenAI embedding API calls to Ollama&#39;s nomic-embed-text</li>
<li><strong>PAM MCP Server</strong> - Serves the MCP tools for accessing and manipulating memories</li>
</ol>
<pre><code class="language-mermaid">graph TD
    VS[VS Code] --&gt; PAM[PAM MCP Server]
    PAM --&gt; EP[Embedding Proxy]
    EP --&gt; OL[Ollama]
    OL --&gt; NOMIC[nomic-embed-text model]
    PAM --&gt; DB[(Memory Databases)]
</code></pre>
<h2>Requirements</h2>
<ul>
<li>Ollama installed and running</li>
<li>mxbai-embed-large model pulled in Ollama</li>
<li>Embedding proxy running on port 3001</li>
<li>PAM MCP server running</li>
</ul>
<h2>Automatic Startup</h2>
<p>The integration components are configured to start automatically:</p>
<ol>
<li><strong>During Development</strong>: Started by <code>start-dev-env.sh</code></li>
<li><strong>System Boot</strong>: Managed by the LaunchAgent <code>com.hextrackr.aimemory.plist</code></li>
</ol>
<h2>Installation</h2>
<p>To install the automatic startup service:</p>
<pre><code class="language-bash">
# Navigate to the scripts directory

cd /Volumes/DATA/GitHub/HexTrackr/scripts

# Run the installation script

./install-ai-memory-service.sh
</code></pre>
<p>This will:</p>
<ul>
<li>Create a LaunchAgent that starts on system boot</li>
<li>Set up proper logging</li>
<li>Register the components in the Memento knowledge graph</li>
</ul>
<h2>Manual Operations</h2>
<p>If you need to manually start or stop the services:</p>
<pre><code class="language-bash">
# Start services

./start-ai-memory-services.sh

# Start through LaunchAgent

launchctl start com.hextrackr.aimemory

# Stop through LaunchAgent

launchctl stop com.hextrackr.aimemory
</code></pre>
<h2>Troubleshooting</h2>
<h3>PAM Tools Showing as Disabled</h3>
<p>If PAM tools show as &quot;disabled by the user&quot; in VS Code:</p>
<ol>
<li><p><strong>Check Embedding Proxy</strong>: Ensure the Ollama embedding proxy is running</p>
<pre><code class="language-bash">curl http://localhost:3001/health
</code></pre>
</li>
<li><p><strong>Check Ollama</strong>: Verify Ollama is running</p>
<pre><code class="language-bash">curl http://localhost:11434/api/version
</code></pre>
</li>
<li><p><strong>Check PAM MCP Server</strong>: Verify the server is running</p>
<pre><code class="language-bash">ps aux | grep mcp_server.py
</code></pre>
</li>
<li><p><strong>Check Logs</strong>:</p>
<pre><code class="language-bash">cat /Volumes/DATA/GitHub/persistent-ai-memory/rmem-prototype/core/proxy.log
cat /Volumes/DATA/GitHub/persistent-ai-memory/mcp_server.log
</code></pre>
</li>
<li><p><strong>Restart Services</strong>:</p>
<pre><code class="language-bash">launchctl unload ~/Library/LaunchAgents/com.hextrackr.aimemory.plist
launchctl load ~/Library/LaunchAgents/com.hextrackr.aimemory.plist
</code></pre>
</li>
</ol>
<h3>Embedding Model Issues</h3>
<p>If you encounter issues with the embedding model:</p>
<ol>
<li><p>Pull the model manually:</p>
<pre><code class="language-bash">ollama pull nomic-embed-text:latest
</code></pre>
</li>
<li><p>Test the model directly:</p>
<pre><code class="language-bash">curl -X POST http://localhost:11434/api/embeddings \
   -d &#39;{&quot;model&quot;:&quot;nomic-embed-text:latest&quot;,&quot;prompt&quot;:&quot;test&quot;}&#39;
</code></pre>
</li>
</ol>
<h2>Configuration</h2>
<p>The system uses the following default configuration:</p>
<ul>
<li>Embedding Proxy Port: 3001</li>
<li>Ollama Port: 11434</li>
<li>Embedding Model: nomic-embed-text:latest</li>
<li>Embedding Dimensions: 768</li>
</ul>
<p>These settings can be modified in the <code>start-ai-memory-services.sh</code> script if needed.</p>
<h2>Memento Integration</h2>
<p>The installation process creates the following entities and relationships in the Memento knowledge graph:</p>
<ul>
<li>Entity: <code>Ollama Embedding Proxy</code> (Service)</li>
<li>Entity: <code>PAM MCP Server</code> (Service)</li>
<li>Entity: <code>AI Memory Service LaunchAgent</code> (Configuration)</li>
<li>Relationship: <code>PAM MCP Server</code> <strong>DEPENDS_ON</strong> <code>Ollama Embedding Proxy</code></li>
<li>Relationship: <code>AI Memory Service LaunchAgent</code> <strong>MANAGES</strong> <code>Ollama Embedding Proxy</code></li>
<li>Relationship: <code>AI Memory Service LaunchAgent</code> <strong>MANAGES</strong> <code>PAM MCP Server</code></li>
</ul>
<p>This documentation can be visualized in the Memento knowledge graph for a better understanding of the system architecture.</p>

    <!-- Note: This file is intentionally minimal. The full page scaffold, header, footer, and scripts
             are provided by docs-html/index.html. Content generated using this template will be
             fetched and injected into #content-container by docs-html/js/docs-portal-v2.js. -->
</section>
