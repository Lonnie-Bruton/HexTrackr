Short answer: your idea is solid, and yes—you can fork a Memory MCP server and add a “protocol memory.” The key is to treat protocols as rules enforced outside the LLM, not just things the model “remembers.” Do that, and your plan becomes very doable even without deep coding chops—if you scope it carefully.

Here’s how I’d think about it.

What’s great about your approach
	•	Cross-project continuity: A single memory layer for projects, prefs, and todos solves the “fragmented context” problem most dev copilots have.
	•	Division of labor: “Ask the Scribe” (LLM on Ollama) handles summarization/reporting while the main build agent stays focused—clean separation of concerns.
	•	Hybrid recall: Symbolic queries (exact fields) + vector search (semantics) + raw transcripts on demand gives both speed and depth.
	•	Protocolization: Turning habits (“git backup before write”) into first-class rules yields consistency and safer automation.

Likely failure modes (and how to blunt them)
	1.	LLM compliance is unreliable.
If protocols live only in prompts, they’ll be ignored under pressure.
→ Fix: Implement a pre/post-action enforcement layer in MCP that intercepts agent tool calls. E.g., any writeFile or git push triggers your “protocol engine” to run git commit && git tag backup/* first. No model “remembering” required.
	2.	Noisy or wrong memories from keyword scraping VS Code chats.
You’ll collect junk (“let’s plan lunch” → roadmap).
→ Fix: Use a two-stage filter: (a) cheap regex/heuristics create candidates; (b) a small classifier LLM (Ollama) validates intent & extracts structured fields. Add confidence + source span to every memory and allow human triage of low-confidence items.
	3.	Memory bloat & drift.
Old and duplicated items swamp retrieval; models hallucinate around stale facts.
→ Fix: Add freshness decay + dedupe (MinHash/LSH or exact-match on normalized summaries) + archival tiers (hot/warm/cold). Periodically summarize long threads into canonical notes.
	4.	Conflicting protocols.
“Always push fast” vs “Run tests before push.”
→ Fix: Protocols need priority, scope, and conflict resolution. Highest-priority rule wins; allow per-workspace overrides; log conflicts.
	5.	Identity & grounding issues.
“tickets.html last Tuesday” has to map to a repo, branch, and file at a specific commit.
→ Fix: Introduce stable IDs and linkage: each memory/todo references {repo, path, branch, commit, ticket id, timestamp}. Add lightweight entity resolution (same file across branches).
	6.	Security & PII sprawl.
Dumping whole debug logs into a DB can leak tokens/keys.
→ Fix: Secret scrubbing (regexes for keys, emails, URLs with tokens) and redaction before persistence; access controls on the DB.

A minimal design that works

1) Protocol Memory (symbolic, enforced)
	•	Schema (SQL):
	•	protocols(id, name, description, condition, action, scope, priority, enabled, created_at, updated_at)
	•	condition: JSONLogic/YAML rule or a tiny DSL
	•	action: list of tool calls (e.g., gitCommit, createBranch, snapshotWorkspace)
	•	scope: global | workspace:{path} | repo:{url}
	•	Engine: When an MCP tool call arrives (e.g., writeFile, refactor, applyPatch), run:
	1.	Select enabled protocols whose condition matches the context.
	2.	Sort by priority.
	3.	Execute actions (idempotently) before allowing the original call.
	4.	Log each enforcement event (auditable).

Example protocol (YAML):

name: backup-before-write
priority: 90
scope: repo:git@github.com:you/yourrepo.git
condition:
  on_tool: ["writeFile", "applyPatch"]
actions:
  - gitCommit: { message: "auto: pre-write backup" }
  - gitTag: { name_prefix: "backup/" }
enabled: true

2) Memory Store (symbolic + vector)
	•	Tables:
	•	notes(id, title, body, type, project, repo, path, ticket_id, created_at, updated_at, source, confidence)
	•	todos(id, text, status, due, project, priority, source_span, created_at)
	•	embeddings(note_id vector) (pgvector/SQLite+FAISS)
	•	Pipelines:
	•	Ingest VS Code chat/debug → heuristics → LLM extractor → structured row (+ embedding).
	•	Nightly summarizer compacts long threads into canonical notes with backlinks to spans.

3) Retrieval
	•	Recall (fast): SQL filters (project/file/date/status) + FTS (titles/notes) + vector top-k (hybrid scoring: alpha*BM25 + (1-alpha)*cosine), boost freshness.
	•	Extended full context: When the agent needs depth, pull the linked raw spans (files, chat excerpts) with chunk boundaries and metadata (speaker, timestamp). Keep chunks small (512–1k tokens), overlap 10–15%.

4) Tools for the agent
	•	memory.search(query, filters) → short results (ids + snippets)
	•	memory.get_full(ids) → raw spans for exact grounding
	•	scribe.search_db(query) → purely symbolic/vector search
	•	scribe.report(query) → LLM-written synthesis over retrieved spans (cite ids)

How to add this to a Memory MCP server
	•	Fork the server and add:
	•	A Protocol middleware that wraps callTool/write-class methods.
	•	New tools:
	•	protocols.list/create/update/toggle
	•	memory.search (filters: project, repo, file, date, tag)
	•	memory.get_full
	•	scribe.report
	•	A Config page / JSON file for default protocols.
	•	Guardrails: block tool calls if mandatory protocols fail (e.g., backup failed) and surface a clear error to the user.

Practical roadmap (bite-sized)
	1.	Week 1: MVP
	•	SQLite + FTS5 for notes/todos, FAISS (or pgvector if Postgres) for embeddings.
	•	One protocol: “backup-before-write” enforced on writeFile.
	•	One ingestion path: VS Code “debug chat” → extractor that makes todo + note.
	•	memory.search tool returning top 10.
	2.	Week 2–3: Hardening
	•	Confidence thresholds + triage queue; dedupe; freshness decay.
	•	Conflict handling & priorities for protocols.
	•	“Ask the Scribe” report tool.
	3.	Week 4+: Quality
	•	Entity resolution for “tickets.html last Tuesday” (resolve repo/branch/file + commit at that date).
	•	Summarization jobs; workspace UI for reviewing memories/protocols.

Answering your exact example

“Hey remember the tickets.html page we worked on last Tuesday…”
	1.	Parse: file=tickets.html, time window=last Tuesday, intent=resume work.
	2.	memory.search filters by path LIKE '%tickets.html%' and created_at within that day, boosted by recent commits.
	3.	If the top result lacks sufficient detail, call memory.get_full to pull the exact debug/chat spans + diffs around that time.
	4.	When the agent proposes a change, protocol engine auto-runs backup before applying the patch.

Is this too ambitious?

Not if you treat it as plumbing, not AI magic, and ship it in layers. The heaviest lift (and the most important) is the deterministic protocol enforcement and the data model; both are well within reach for a network engineer comfortable with YAML, SQL, and basic TypeScript/Python.

If you want, I can sketch a tiny SQLite schema plus a reference “backup-before-write” middleware for an MCP tool handler next.